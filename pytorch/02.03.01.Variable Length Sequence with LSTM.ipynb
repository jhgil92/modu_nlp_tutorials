{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1_x = ['is', 'it', 'too', 'late', 'now', 'say', 'sorry']\n",
    "sent_1_y = ['VB', 'PRP', 'RB', 'RB', 'RB', 'VB', 'JJ']\n",
    "\n",
    "sent_2_x = ['ooh', 'ooh']\n",
    "sent_2_y = ['NNP', 'NNP']\n",
    "\n",
    "sent_3_x = ['sorry', 'yeah']\n",
    "sent_3_y = ['JJ', 'NNP']\n",
    "\n",
    "X = [sent_1_x, sent_2_x, sent_3_x]\n",
    "Y = [sent_1_y, sent_2_y, sent_3_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'it', 'too', 'late', 'now', 'say', 'sorry'],\n",
       " ['ooh', 'ooh'],\n",
       " ['sorry', 'yeah']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['VB', 'PRP', 'RB', 'RB', 'RB', 'VB', 'JJ'], ['NNP', 'NNP'], ['JJ', 'NNP']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map sentences to vocab\n",
    "vocab = {'': 0, 'is': 1, 'it': 2, 'too': 3, 'late': 4, 'now': 5, 'say': 6, 'sorry': 7, 'ooh': 8, 'yeah': 9} \n",
    "\n",
    "# fancy nested list comprehension\n",
    "X =  [[vocab[word] for word in sentence] for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 2]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of each sentence\n",
    "X_lengths = [len(sentence) for sentence in X]\n",
    "X_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty matrix with padding tokens\n",
    "pad_token = vocab['']\n",
    "longest_sent = max(X_lengths)\n",
    "batch_size = len(X)\n",
    "padded_X = np.ones((batch_size, longest_sent), dtype=int) * pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths):\n",
    "  sequence = X[i]\n",
    "  padded_X[i, 0:x_len] = sequence[:x_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6, 7],\n",
       "       [8, 8, 0, 0, 0, 0, 0],\n",
       "       [7, 9, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-d1916669c283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fancy nested list comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Y now looks like:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-d1916669c283>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fancy nested list comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Y now looks like:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-d1916669c283>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fancy nested list comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Y now looks like:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tags = {'': 0, 'VB': 1, 'PRP': 2, 'RB': 3, 'JJ': 4, 'NNP': 5}\n",
    "\n",
    "# fancy nested list comprehension\n",
    "Y =  [[tags[tag] for tag in sentence] for sentence in Y]\n",
    "\n",
    "# Y now looks like:\n",
    "# [[1, 2, 3, 3, 3, 1, 4], [5, 5], [4, 5]]\n",
    "\n",
    "# get the length of each sentence\n",
    "Y_lengths = [len(sentence) for sentence in Y]\n",
    "\n",
    "# create an empty matrix with padding tokens\n",
    "pad_token = tags['']\n",
    "longest_sent = max(Y_lengths)\n",
    "batch_size = len(Y)\n",
    "padded_Y = np.ones((batch_size, longest_sent)) * pad_token\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, y_len in enumerate(Y_lengths):\n",
    "  sequence = Y[i]\n",
    "  padded_Y[i, 0:y_len] = sequence[:y_len]\n",
    "\n",
    "# # padded_Y looks like:\n",
    "# array([[ 1.,  2.,  3.,  3.,  3.,  1.,  4.],\n",
    "#        [ 5.,  5.,  0.,  0.,  0.,  0.,  0.],\n",
    "#        [ 4.,  5.,  0.,  0.,  0.,  0.,  0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 3., 3., 1., 4.],\n",
       "       [5., 5., 0., 0., 0., 0., 0.],\n",
       "       [4., 5., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = torch.nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=3,\n",
    "            padding_idx=pad_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.from_numpy(padded_X)#.to(device)\n",
    "X_lengths_tensor = torch.from_numpy(np.array(X_lengths))#.to(device)\n",
    "\n",
    "# # batch_size, seq_len, _ = padded_X.size()\n",
    "# X_embed = word_embedding(X_tensor)\n",
    "\n",
    "# X_pack_padded = torch.nn.utils.rnn.pack_padded_sequence(X_embed, np.array(X_lengths), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7],\n",
       "        [8, 8, 0, 0, 0, 0, 0],\n",
       "        [7, 9, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 2])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lengths_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed = word_embedding(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1799,  0.3167,  1.1528],\n",
       "         [-1.1772, -0.5380,  0.8375],\n",
       "         [-0.4743,  1.3381,  1.4349],\n",
       "         [ 1.6677,  0.6381, -0.3952],\n",
       "         [ 1.6498,  0.5117, -0.9820],\n",
       "         [-0.4703,  1.4595, -0.3970],\n",
       "         [-1.1577,  0.4299, -0.2120]],\n",
       "\n",
       "        [[ 0.6767, -1.2970, -1.3783],\n",
       "         [ 0.6767, -1.2970, -1.3783],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.1577,  0.4299, -0.2120],\n",
       "         [ 0.6238,  0.2805,  0.9411],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pack_padded = torch.nn.utils.rnn.pack_padded_sequence(X_embed, np.array(X_lengths), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1799,  0.3167,  1.1528],\n",
       "        [ 0.6767, -1.2970, -1.3783],\n",
       "        [-1.1577,  0.4299, -0.2120],\n",
       "        [-1.1772, -0.5380,  0.8375],\n",
       "        [ 0.6767, -1.2970, -1.3783],\n",
       "        [ 0.6238,  0.2805,  0.9411],\n",
       "        [-0.4743,  1.3381,  1.4349],\n",
       "        [ 1.6677,  0.6381, -0.3952],\n",
       "        [ 1.6498,  0.5117, -0.9820],\n",
       "        [-0.4703,  1.4595, -0.3970],\n",
       "        [-1.1577,  0.4299, -0.2120]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pack_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "max_length = 3  #longest_sent  # 7  이걸로 하면 에러난다.\n",
    "hidden_size = 3\n",
    "n_layers =1\n",
    "\n",
    "# initialize\n",
    "rnn = nn.RNN(max_length, hidden_size, n_layers, batch_first=True)     # (중요) batch_first=True\n",
    "h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))\n",
    "\n",
    "#forward \n",
    "out, _ = rnn(X_pack_padded, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.8422, -0.5469,  0.6166],\n",
       "        [ 0.2324, -0.7649,  0.8579],\n",
       "        [ 0.8054, -0.9036,  0.7375],\n",
       "        [ 0.7346, -0.7846,  0.7465],\n",
       "        [ 0.0186, -0.8162,  0.7977],\n",
       "        [ 0.3019, -0.8325,  0.2773],\n",
       "        [ 0.7203, -0.9097,  0.1353],\n",
       "        [-0.2071, -0.8956,  0.0162],\n",
       "        [ 0.0606, -0.8438,  0.4753],\n",
       "        [ 0.6802, -0.9495,  0.3824],\n",
       "        [ 0.6285, -0.9241,  0.5867]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8422, -0.5469,  0.6166],\n",
       "         [ 0.2324, -0.7649,  0.8579],\n",
       "         [ 0.8054, -0.9036,  0.7375]],\n",
       "\n",
       "        [[ 0.7346, -0.7846,  0.7465],\n",
       "         [ 0.0186, -0.8162,  0.7977],\n",
       "         [ 0.3019, -0.8325,  0.2773]],\n",
       "\n",
       "        [[ 0.7203, -0.9097,  0.1353],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2071, -0.8956,  0.0162],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0606, -0.8438,  0.4753],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.6802, -0.9495,  0.3824],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.6285, -0.9241,  0.5867],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
